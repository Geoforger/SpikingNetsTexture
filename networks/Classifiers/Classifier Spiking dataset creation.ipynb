{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f89d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "#from collections import OrderedDict\n",
    "#import sklean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cde1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "textures = 11\n",
    "trials = 100\n",
    "\n",
    "# Set bin and sim params\n",
    "#bin_size = 10   # Bin size in ms\n",
    "bin_sizes = [10, 100, 200, 250, 300] # Bin sizes for different datasets\n",
    "sim_length = 5000   # Sim time in ms\n",
    "#bins = math.ceil(sim_length / bin_size)   # Round number of bins up to the nearest integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173305d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove repeated spikes from data\n",
    "# def remove_duplicates(data):\n",
    "#     for i in range(len(data)):\n",
    "#         for j in range(len(data[i])): \n",
    "#             data[i][j] = list(OrderedDict.fromkeys(data[i][j]))\n",
    "#     return data\n",
    "\n",
    "# New numpy based function\n",
    "def remove_duplicates(data):\n",
    "    new_data = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data[i]:\n",
    "            new_data.append(np.unique(data[i]))\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010304d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an array of labels for the dataset\n",
    "# Returns a 1d list of texture labels\n",
    "# NOTE: This function only works when dataset is creted as below\n",
    "def create_labels(no_textures, no_trials):\n",
    "    labels = []\n",
    "    \n",
    "    for q in range(no_textures):\n",
    "        for w in range(trials):\n",
    "            labels.append(q)\n",
    "            \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248b62d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size = 10\n",
      "Bin Size = 100\n",
      "Bin Size = 200\n",
      "Bin Size = 250\n",
      "Bin Size = 300\n"
     ]
    }
   ],
   "source": [
    "# Import and bin data\n",
    "\n",
    "\n",
    "# Loop for all bin_sizes\n",
    "for x in bin_sizes:\n",
    "    \n",
    "    print(\"Bin Size = \" + str(x))    \n",
    "    bins = math.ceil(sim_length / x)   # Round number of bins up to the nearest integer\n",
    "    dataset = np.empty([int(bins)])\n",
    "\n",
    "    for kk in range(textures):\n",
    "        for ll in range(trials):\n",
    "            # Path to file containing spike timings\n",
    "            FILE_NAME = \"Artificial Dataset \" + str(ll) + \"Texture No. \" + str(kk) + \".pickle\"\n",
    "            DATA_PATH = \"/home/farscope2/Documents/PhD/Spiking Nets Project/SpikingNetsTexture/datasets/TacTip_NM/Reduced_natural/\" + FILE_NAME\n",
    "\n",
    "            # Import and flatten the dataset for use in the network\n",
    "            spike_times = np.load(DATA_PATH, allow_pickle=True)\n",
    "            spks = spike_times.reshape(-1)\n",
    "\n",
    "            # Remove duplicates using previous function\n",
    "            spks = remove_duplicates(spks)\n",
    "\n",
    "            # List to hold all data from item\n",
    "            data = []\n",
    "            value = 0\n",
    "\n",
    "            # Loop untill all bins are full\n",
    "            for bin_ms in range(0, sim_length, x):\n",
    "                # Loop entire population\n",
    "                for t in range(len(spks)):\n",
    "                    if spks[t].any():\n",
    "                        # Check number of times each neuron spiked within this timeframe\n",
    "                        ar = np.array(spks[t])\n",
    "                        value += np.count_nonzero(((ar >= bin_ms) & (ar < bin_ms + x)))\n",
    "\n",
    "                data.append(value)\n",
    "                value = 0\n",
    "\n",
    "            # DEBUG\n",
    "            #print(\"Trial: \" + str(ll) + \"Texture: \" + str(kk))\n",
    "            #print(\"Bin Size =\" +str(x))\n",
    "            #print(dataset)\n",
    "            #print(data)\n",
    "\n",
    "            # If dataset is currently empty then dataset is formed from data list\n",
    "            # Else create row below current dataset to input new data\n",
    "            if dataset.size == 0:\n",
    "                dataset[0] = np.array(data)\n",
    "            else:\n",
    "                dataset = np.vstack([dataset, data])\n",
    "\n",
    "\n",
    "            data_dir = \"/home/farscope2/Documents/PhD/Spiking Nets Project/SpikingNetsTexture/datasets/TacTip_NM/natural_classifier\"\n",
    "            pickle_out = open(os.path.join(data_dir, str(sim_length) + \"ms - \" + str(x) + \"ms bin size dataset.pickle\"), 'wb')\n",
    "            pickle.dump(dataset, pickle_out)\n",
    "            pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f40de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label list for dataset\n",
    "labels = create_labels(textures, trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a112ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the dataset for ease of future use\n",
    "# This part of the notebook need only be used once\n",
    "\n",
    "# data_dir = \"/home/farscope2/Documents/PhD/Spiking Nets Project/SpikingNetsTexture/datasets/TacTip_NM/natural_classifier\"\n",
    "# pickle_out = open(os.path.join(data_dir, str(sim_length) + \"ms - \" + str(bin_size) + \"ms bin size dataset.pickle\"), 'wb')\n",
    "# pickle.dump(dataset, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(data_dir, str(textures) + \" textures - \" + str(trials) + \" trials labels.pickle\"), 'wb')\n",
    "pickle.dump(labels, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca1fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
